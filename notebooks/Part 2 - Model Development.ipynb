{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Sentiment Analysis"
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting wordcloud\n  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 366 kB 20.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from wordcloud) (1.18.5)\nRequirement already satisfied: matplotlib in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from wordcloud) (3.2.2)\nRequirement already satisfied: pillow in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from wordcloud) (7.2.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.2.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.1)\nRequirement already satisfied: six in /opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\nInstalling collected packages: wordcloud\nSuccessfully installed wordcloud-1.8.1\n"
                }
            ],
            "source": "# Install required packages\n!pip install wordcloud"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": "[nltk_data] Downloading package punkt to /home/wsuser/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
                }
            ],
            "source": "# Import required libraries\n\nimport itertools\nimport os\nimport requests\nimport tarfile\nfrom collections import defaultdict\n\nimport string\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Table of Contents\n\n* [1. Sentiment Analysis](#sentiment)\n    * [1.1 Load Data](#1-1)\n    * [1.2 Unigrams](#1-2)\n    * [1.3 Bigrams](#1-3)\n    * [1.4 Using Composition and Adjective Classes](#1-4)\n    * [1.5 Combining Unigram, Bigram, Component/Adj Classes](#1-5)\n    * [1.6 Group by Overall Sentiment](#1-6)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# 1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment\"></a>\nFor this section, we use [IBM's Debater - Sentiment Composition Lexicons](https://developer.ibm.com/exchanges/data/all/sentiment-composition-lexicons/) dataset. Our goal here is to take text and capture sentiment on the entire text as well as for each sentence in the text. We base our model on [this paper](https://www.aclweb.org/anthology/C18-1189.pdf) (by creators of this dataset). Our final model is section 1.5, which combines 1.2 to 1.4. So in the following subsections, we:\n\n* 1.1 Load Data\n* 1.2 Create method that uses 'LEXICON_UG.txt' to match unigrams to sentiment score\n* 1.3 Create method that uses 'LEXICON_BG.txt' to match bigrams to sentiment score\n* 1.4 Create method that uses the rules from Table 1 of the paper to produce sentiment scores. This method essentially matches bigrams to certain rules that produce a predicted polarity (positive or negative)\n* 1.5 Combines 1.2 to 1.4. \n    * We first get bigrams of the text. Then to determine the sentiment of each bigram we will:\n        1. Take the bigram score (1.2, calulate_bigram_sentiment()). If this does not exist then,\n        2. Take the score from matching component/adjective classes (1.3, calculate_compostion_or_adj_sentiment()). If this does not exist then,\n        3. Look at the unigrams (1.1, calulate_bigram_sentiment()) of the bigram. Both words need to be negative in order to be negative (similarly for positive). If one is positive and one negative then it is neutral\n        * Positive bigram score = +1, Negative bigram score = -1, Neutral bigram score = 0\n    * To determine final sentiment of a sentence, the sentiment score of each bigram is added together.\n    * To determine final sentiment of entire text, the sentiment score of each sentence is added together. \n* 1.6 Using 1.5, demonstrate examples of grouping comments by sentiment as well as hilighting sentiment on a sentence level."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1.1 Load Data <a class=\"anchor\" id=\"1-1\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "def download_data(url_base, version, data_file_name):\n    # Downloading the dataset\n    url = \"{}/{}/{}\".format(url_base, version, data_file_name)\n    response = requests.get(url)\n\n    # Check for errors\n    if not response.ok:\n        print(\"There are some errors when downloading {}\".format(url))\n\n    # Open tar file\n    with open(data_file_name, 'wb') as file_name:\n        file_name.write(response.content)\n    \ndef extract_data(data_directory, data_file_name):\n    # Extracting the dataset\n    with tarfile.open(data_file_name) as file_name:\n        file_name.extractall(path='./' + data_directory)"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "# Download the dataset\nsentiment_data_directory = 'data/sentiment-composition-lexicons'\nurl_base = 'https://dax-cdn.cdn.appdomain.cloud/dax-sentiment-composition-lexicons'\nversion = '1.0.2'\ndata_file_name = 'sentiment-composition-lexicons.tar.gz'\ndownload_data(url_base, version, data_file_name)"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "# Extract the dataset\nextract_data(sentiment_data_directory, data_file_name)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "# Comments for testing\ncomments_5 = [\n    'Customer service was polite.',\n    'The socks are a pretty color but expensive.',\n    'The shirt I bought was green and service was great.',\n    'I think the sweater and socks were perfect.',\n    'I do not like the shoes, so ugly and expensive.',\n]"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1.2 Unigrams <a class=\"anchor\" id=\"1-2\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "  UNIGRAM  SENTIMENT_SCORE  sentiment\n0      aa         0.019674          1\n1     aaa         0.032775          1\n2    aaas         0.074593          1\n3  aachen         0.011926          1\n4     aah         0.118070          1\n"
                }
            ],
            "source": "# Read unigram data\nunigram_df = pd.read_csv(os.path.join(sentiment_data_directory, 'LEXICON_UG.txt'), sep=\" \")\n\n# Add sentiment column\nunigram_df['sentiment'] = np.where(unigram_df['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\nprint(unigram_df.head())\n\n# Create dict with unigram and sentiment\nunigram_sentiment_dict = pd.Series(unigram_df.SENTIMENT_SCORE.values,\n                                   index=unigram_df.UNIGRAM.values).to_dict()"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "# Simple implementation: if finds token in unigram_sentiment_dict, then adds sentiment\n# The total sentiment for sentence is averaged\ndef calculate_unigram_sentiment(tokenized_sentence, sentiment_map=unigram_sentiment_dict):\n    sentiment_score = 0\n    for token in tokenized_sentence:\n        token_sentiment = sentiment_map.get(token)\n        if token_sentiment is None:\n            continue\n        else:\n            sentiment_score += token_sentiment\n\n    return sentiment_score"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Score: 0.7981163, Sentence: Customer service was polite.\nScore: -0.26573172, Sentence: The socks are a pretty color but expensive.\nScore: 0.18411419999999995, Sentence: The shirt I bought was green and service was great.\nScore: 0.16159839999999998, Sentence: I think the sweater and socks were perfect.\nScore: -1.1203735, Sentence: I do not like the shoes, so ugly and expensive.\n"
                }
            ],
            "source": "# Print out example sentences and their sentiment.\n# The more negative the score, the more negative the sentiment.\n# The more positive the score, the more positive the sentiment.\nfor sentence in comments_5:\n    score = calculate_unigram_sentiment(word_tokenize(sentence.lower()))\n    print('Score: {}, Sentence: {}'.format(score, sentence))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1.3 Bigrams <a class=\"anchor\" id=\"1-3\"></a>"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BIGRAM</th>\n      <th>POS_TAGS</th>\n      <th>SENTIMENT_SCORE</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>abalone-divers</td>\n      <td>NN-NNS</td>\n      <td>-0.090230</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>abandoned-animals</td>\n      <td>VBN-NNS</td>\n      <td>-0.089895</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>abandoned-apartment</td>\n      <td>VBN-NN</td>\n      <td>-0.126907</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>abandoned-attempts</td>\n      <td>VBN-NNS</td>\n      <td>-0.053709</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>abandoned-babies</td>\n      <td>VBN-NNS</td>\n      <td>-0.074742</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "                BIGRAM POS_TAGS  SENTIMENT_SCORE  sentiment\n0       abalone-divers   NN-NNS        -0.090230          0\n1    abandoned-animals  VBN-NNS        -0.089895          0\n2  abandoned-apartment   VBN-NN        -0.126907          0\n3   abandoned-attempts  VBN-NNS        -0.053709          0\n4     abandoned-babies  VBN-NNS        -0.074742          0"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# Read bigram data\nbigrams_df = pd.read_csv(os.path.join(sentiment_data_directory, 'LEXICON_BG.txt'), sep=\" \")\n\n# Add sentiment column\nbigrams_df['sentiment'] = np.where(bigrams_df['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\nbigrams_df.head()"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": "bigram_sentiment_dict = pd.Series(bigrams_df.SENTIMENT_SCORE.values,\n                                   index=bigrams_df.BIGRAM.str.split('-').apply(lambda l: tuple(l))).to_dict()"
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "def calculate_bigram_sentiment_sentence(sentence_bigrams):\n    bigrams = []\n    bigram_sentiment_score = 0\n    for bigram in sentence_bigrams:\n        bigram_sentiment = calculate_bigram_sentiment(bigram)\n        if bigram_sentiment is not None:\n            bigram_sentiment_score += bigram_sentiment\n            bigrams.append(bigram)\n    \n    return bigram_sentiment_score, bigrams\n\n\ndef calculate_bigram_sentiment(bigram):\n    bigram_sentiment = bigram_sentiment_dict.get(bigram)\n    if bigram_sentiment:\n        # -0.02 and 0.02 allows for margin of error for neutrals\n        if bigram_sentiment < -0.02:\n            return -1\n        elif bigram_sentiment > 0.02:\n            return 1\n        else:\n            return 0\n    return None"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(1, [('accept', 'payments')])"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sentence = 'The high prices are ridiculous'\nsentence2 = 'They do not accept payments from my credit card'\ncalculate_bigram_sentiment_sentence(list(nltk.bigrams(word_tokenize(sentence.lower()))))\ncalculate_bigram_sentiment_sentence(list(nltk.bigrams(word_tokenize(sentence2.lower()))))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1.4 Using Composition and Adjective Classes <a class=\"anchor\" id=\"1-4\"></a>"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Here, we will uses the rules from Table 1 of [this paper](https://www.aclweb.org/anthology/C18-1189.pdf) (by creators of this dataset), to produce sentiment scores. We will essentially match bigrams to certain rules that produce a predicted polarity (positive or negative). There are two groups of rules: composition classes and adjective classes. Adjective classes focus on the adjective pairs (high, low) and (fast, slow).\n\nIn order to do this, there are two files: 1) ADJECTIVES.xlsx and 2) SEMANTIC_CLASSES.xlsx. The adjectives files contains 5 sheets, the first sheet gives a list of words similar to each of high, low, fast, slow. The next four sheets are words that are associated with that specific case. \n\nThe semantic classes file has 6 sheets, one for each of the composition classes defined in the paper. In each sheet, there is a list of words that corresponds to that composition class.\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.4.1 Reading Adjective Classes Data Files\n\nFirst, we will read in ADJECTIVE_EXPANSION.xlsx and clean it up."
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "xls_file = pd.ExcelFile(os.path.join(sentiment_data_directory, 'ADJECTIVES.xlsx'))\nadjective_expansion = pd.read_excel(xls_file, 'ADJECTIVE_EXPANSION').dropna(how='all').reset_index(drop=True)\nhigh_low_PN = pd.read_excel(xls_file, '(HIGH,LOW)_POS_NEG', header=None)[0].values.tolist()\nhigh_low_NP = pd.read_excel(xls_file, '(HIGH,LOW)_NEG_POS', header=None)[0].values.tolist()\nfast_slow_PN = pd.read_excel(xls_file, '(FAST,SLOW)_POS_NEG', header=None)[0].values.tolist()\nfast_slow_NP = pd.read_excel(xls_file, '(FAST,SLOW)_NEG_POS', header=None)[0].values.tolist()\n\n\ndef clean_adjective_df(df):\n    adjectives = []\n    for i in range(4):\n        adjectives.extend(df.iloc[:, i+1].dropna().tolist())\n    \n    adj_category = df.iloc[0,0]\n    \n    return [(adj, adj_category) for adj in adjectives]\n\n\ntokens = []\ntoken_rows = 5\nfor i in range(0, len(adjective_expansion), token_rows+1):\n    tokens.append(clean_adjective_df(adjective_expansion.loc[i:i+token_rows]))\nhigh_tokens, low_tokens, fast_tokens, slow_tokens = tokens\n\nadjective_class_map = dict(high_tokens + low_tokens + fast_tokens + slow_tokens)"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "{'FAST_TOKENS', 'HIGH_TOKENS', 'LOW_TOKENS', 'SLOW_TOKENS'}"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "set(adjective_class_map.values())"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Now we have a dictionary that matches words to the adjective class (fast, high, low, slow). The dictionary looks like:\n{word: adjective_class}"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.4.2 Reading Compostion Classes Data Files\n\nNext, we read the SEMANTIC_CLASSES.xlsx file"
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": "semantic_classes_file = pd.ExcelFile(os.path.join(sentiment_data_directory, 'SEMANTIC_CLASSES.xlsx'))\n\ndominator_neg = pd.read_excel(semantic_classes_file, 'DOMINATOR_NEG', header=None)[0].values.tolist()\ndominator_pos = pd.read_excel(semantic_classes_file, 'DOMINATOR_POS', header=None)[0].values.tolist()\npropagator_pos = pd.read_excel(semantic_classes_file, 'PROPAGATOR_POS', header=None)[0].values.tolist()\npropagator_neg = pd.read_excel(semantic_classes_file, 'PROPAGATOR_NEG', header=None)[0].values.tolist()\nreverser_pos = pd.read_excel(semantic_classes_file, 'REVERSER_POS', header=None)[0].values.tolist()\nreverser_neg = pd.read_excel(semantic_classes_file, 'REVERSER_NEG', header=None)[0].values.tolist()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### 1.4.3 Matching Adjective/Composition Classes\nNow we write a method to match bigrams to an adjective or composition class. The bigram matching order will be, as stated in the paper: ADJ (adjective), REV (reverse), PROP (propagator), DOM (dominator)"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": "class Adjective:\n    FAST = 'FAST_TOKENS'\n    SLOW = 'SLOW_TOKENS'\n    HIGH = 'HIGH_TOKENS'\n    LOW = 'LOW_TOKENS'\n    \nclass Sign:\n    POSITIVE = 'Positive'\n    NEGATIVE = 'Negative'\n    NEUTRAL = 'Neutral'\n\nadjective_conditions = {\n    Adjective.FAST: [(fast_slow_PN, Sign.POSITIVE), (fast_slow_NP, Sign.NEGATIVE)],\n    Adjective.SLOW: [(fast_slow_PN, Sign.NEGATIVE), (fast_slow_NP, Sign.POSITIVE)],\n    Adjective.HIGH: [(high_low_PN, Sign.POSITIVE), (high_low_NP, Sign.NEGATIVE)],\n    Adjective.LOW: [(high_low_PN, Sign.NEGATIVE), (high_low_NP, Sign.POSITIVE)],\n}\n\nsentiment_to_score = {\n    Sign.POSITIVE: +1,\n    Sign.NEGATIVE: -1,\n}\n\ndef is_given_sentiment(sentiment, word, sentiment_map):\n    if word in sentiment_map:\n        if sentiment==Sign.NEGATIVE and sentiment_map[word] < 0:\n            return True\n        elif sentiment==Sign.POSITIVE and sentiment_map[word] > 0:\n            return True\n\ndef calculate_composition_or_adj_sentiment(bigram):\n    # Adjective\n    adjective_token = adjective_class_map.get(bigram[0])\n    if adjective_token is not None:\n        for expansions_list, sentiment_sign in adjective_conditions[adjective_token]:\n            if bigram[1] in expansions_list:\n                return sentiment_to_score[sentiment_sign]\n\n    # Composition: Reverser\n    elif bigram[0] in reverser_pos and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.POSITIVE]\n    elif bigram[0] in reverser_neg and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.NEGATIVE]\n\n    # Composition: Propagator\n    elif bigram[0] in propagator_pos and is_given_sentiment(Sign.NEGATIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.POSITIVE]\n    elif bigram[0] in propagator_neg and is_given_sentiment(Sign.POSITIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.NEGATIVE]\n\n    # Composition: Dominator\n    elif bigram[0] in dominator_neg:\n        return sentiment_to_score[Sign.NEGATIVE]\n    elif bigram[0] in dominator_pos:\n        return sentiment_to_score[Sign.POSITIVE]\n    \n    return None\n    \n        \ndef calculate_composition_or_adj_sentiment_sentence(sentence):\n    sentiment_count = 0\n    bigrams = []\n    sentence_bigrams = list(nltk.bigrams(word_tokenize(sentence.lower())))\n    for bigram in sentence_bigrams:\n        sentiment = calculate_composition_or_adj_sentiment(bigram)\n        if sentiment is not None:\n            sentiment_count += sentiment\n            bigrams.append(bigram)\n        else:\n            continue \n            \n    # if sentiment_count > 0 then positive\n    return sentiment_count, bigrams"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "We can now calculate the sentiment of sentences. For example:"
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "(-1, [('high', 'prices')])"
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "calculate_composition_or_adj_sentiment_sentence(\"The high prices are ridiculous\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "A value < 0 means a negative sentiment and a value > 0 means positive. Thus, in the example above, that sentence has a negative sentiment. The tuple printed out is the bigram that was matched to determine the sentiment."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 1.5 Combining Unigram, Bigram, Component/Adj Classes <a class=\"anchor\" id=\"1-5\"></a>\n\nIn this section, we combine the techniques from 1.1 to 1.3 to calculate sentiment. We will first get bigrams of the text. Then ti determine the final sentiment of each bigram we will:\n1. Take the bigram score (1.2, calulate_bigram_sentiment()). If this does not exist then,\n2. Take the score from matching component/adj (1.3, calculate_compostion_or_adj_sentiment()). If this does not exist then,\n3. Look at the unigrams (1.1, calulate_bigram_sentiment()) of the bigram. Both words need to be negative in order to negative (similar for positive). If one is positive and one negative then it is neutral"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": "def calculate_sentiment_combined(sentence):\n    sentiment_score = 0\n    table = str.maketrans(dict.fromkeys(string.punctuation))\n    cleaned_sentence = sentence.translate(table)  # remove punctuation\n    sentence_bigrams = list(nltk.bigrams(word_tokenize(cleaned_sentence.lower())))\n    \n    for bigram in sentence_bigrams:\n        current_sentiment = calculate_bigram_sentiment(bigram)\n        if current_sentiment is None:\n            current_sentiment = calculate_composition_or_adj_sentiment(bigram)\n            if current_sentiment is None:\n                unigram_sentiment = calculate_unigram_sentiment(bigram)\n                if unigram_sentiment < - 0.1:\n                    current_sentiment = -1\n                elif unigram_sentiment > 0.1:\n                    current_sentiment = 1\n                else:\n                    current_sentiment = 0\n        \n        sentiment_score += current_sentiment\n    \n    return sentiment_score\n    "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Example using the combined model. As stated previously, a score < 0 is negative sentiment, score > 0 is positive, and score = 0 is neutral."
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Score: 2, Sentence: Customer service was polite.\nScore: -1, Sentence: The socks are a pretty color but expensive.\nScore: -1, Sentence: The shirt I bought was green and service was great.\nScore: -1, Sentence: I think the sweater and socks were perfect.\nScore: -3, Sentence: I do not like the shoes, so ugly and expensive.\nScore: -3, Sentence: The high prices are ridiculous\nScore: 0, Sentence: They do not accept payments from my credit card\nScore: 7, Sentence: I absolutely love how nice they are, I would definitely buy again from here.\nScore: -2, Sentence: I hate their products, so horrible, I cannot believe I spent so much money on shoes.\nScore: 3, Sentence: However, their webiste is beautiful and modern.\n"
                }
            ],
            "source": "extra_sentences = ['The high prices are ridiculous', \n                   'They do not accept payments from my credit card',\n                   'I absolutely love how nice they are, I would definitely buy again from here.',\n                   'I hate their products, so horrible, I cannot believe I spent so much money on shoes.',\n                   'However, their webiste is beautiful and modern.',\n                  ]\nsentiment_sentences = comments_5 + extra_sentences\n\nfor sentence in sentiment_sentences:\n    score = calculate_sentiment_combined(sentence)\n    print('Score: {}, Sentence: {}'.format(score, sentence))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## 2.6 Group by Overall Sentiment <a class=\"anchor\" id=\"1-5\"></a>\nGroup by the comment's overall sentiment. Within each comment, label the postive/negative sentences."
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": "def convert_score_to_sentiment(score):\n    if score < 0:\n        return Sign.NEGATIVE\n    elif score > 0:\n        return Sign.POSITIVE\n    else:\n        return Sign.NEUTRAL\n    \n\ndef calculate_sentence_level_sentiment(comment_by_sentence):\n    \"\"\"\n    :param comment_by_sentence: comment broken down by sentence [sentence1, sentence2, ...]\n                                each sentence is string.\n    :return: (overall_score, [(sentiment, sentence), ...])\n    \"\"\"\n    sentence_level_sentiment = []\n    overall_score = 0\n    for sentence in comment_by_sentence:\n        score = calculate_sentiment_combined(sentence)\n        overall_score += score\n        sentiment_sentence_pair = (convert_score_to_sentiment(score), sentence)\n        sentence_level_sentiment.append(sentiment_sentence_pair)\n    return overall_score, sentence_level_sentiment\n    \n\ndef group_comments_by_sentiment(comments):\n    \"\"\"\n    :param comments: [comment, comment, ...]\n    :return: {Pos: [[(+/-, sentence1), (+/-, sentence2), ...], [...], [...], ...]\n              Neg: [], Neutral: [], }\n    \"\"\"\n    overall_sentiment = defaultdict(list)\n    for comment in comments:\n        comment_by_sentence = nltk.tokenize.sent_tokenize(comment)\n        overall_score, sentence_level_sentiment = calculate_sentence_level_sentiment(comment_by_sentence)\n        overall_sentiment[convert_score_to_sentiment(overall_score)].append(sentence_level_sentiment)\n\n    return overall_sentiment"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Example"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Negative:\n(-) I bought several items: socks, shirt, sweater.\n(-) By far my most favorite was the shirt because it is so soft.\n(-) However, the sweater and socks missed the mark.\n\n(-) Horrible, horrible customer service, I have never met such rude people.\n(-) Why is it so bad?\n(-) Would not recommend at all.\n\nPositive:\n(-) My order arrived several days late.\n(+) But when I contacted customer serivce they were very helpful and refunded me.\n\n(+) I bought somethings on sale, they were a great deal.\n(=) Will be buying more next time.\n\nNeutral:\n(-) Everything I ordered arrived on time and looked exactly like in the pictures!\n(+) This company has high quality products.\n\n"
                }
            ],
            "source": "comments = [\n    'I bought several items: socks, shirt, sweater. By far my most favorite was the shirt because it is so soft. However, the sweater and socks missed the mark.',\n    'My order arrived several days late. But when I contacted customer serivce they were very helpful and refunded me.',\n    'Horrible, horrible customer service, I have never met such rude people. Why is it so bad? Would not recommend at all.',\n    'Everything I ordered arrived on time and looked exactly like in the pictures! This company has high quality products.',\n    'I bought somethings on sale, they were a great deal. Will be buying more next time.'\n]\n\noverall_sentiment = group_comments_by_sentiment(comments)\nsign_word_to_symbol = {Sign.NEGATIVE: '-', Sign.POSITIVE: '+', Sign.NEUTRAL: '='}\n\n# print out results in readible way\nfor sign in overall_sentiment:\n    print('{}:'.format(sign))\n    for comment in overall_sentiment[sign]:\n        sentence_printout_text = ''\n        for sentence_sign, sentence in comment:\n            sentence_printout_text += '({}) {}\\n'.format(sign_word_to_symbol[sentence_sign], sentence)\n        print(sentence_printout_text)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.7",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}