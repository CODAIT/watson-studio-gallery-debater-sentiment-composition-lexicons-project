{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Sentiment Analysis with IBM Debater Sentiment Composition Lexicons Dataset"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook relates to the IBM Debater\u00ae Sentiment Composition Lexicons dataset. The dataset includes sentiment composition lexicons and sentiment lexicons:\n\n- Sentiment composition lexicons containing 2,783 words.\n- Sentiment lexicons containing 66,058 unigrams and 262,555 bigrams.\n\nThis dataset can be obtained for free from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/sentiment-composition-lexicons/).\n\nIn this notebook, we will explore how to infer sentiments from a document using the dataset."}, {"metadata": {}, "cell_type": "markdown", "source": "# Table of Contents\n\n* [0. Prerequisites](#0)\n* [1. Sentiment Analysis](#sentiment)\n    * [1.1 Load Data](#1-1)\n    * [1.2 Unigrams](#1-2)\n    * [1.3 Bigrams](#1-3)\n    * [1.4 Using Composition and Adjective Classes](#1-4)\n    * [1.5 Combining Unigram, Bigram, Component/Adj Classes](#1-5)\n    * [1.6 Group by Overall Sentiment](#1-6)"}, {"metadata": {}, "cell_type": "markdown", "source": "## 0. Prerequisites <a class=\"anchor\" id=\"0\"></a>\n\nBefore you run this notebook complete the following steps:\n- Insert a project token\n- Import required modules\n\n#### Insert a project token\n\nWhen you import this project from the Watson Studio Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n\n```python\n# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\npc = project.project_context\n```\n\nIf you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n\n* Click on `More -> Insert project token` in the top-right menu section\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n* This should insert a cell at the top of this notebook similar to the example given above.\n\n  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n\n* Run the newly inserted cell before proceeding with the notebook execution below"}, {"metadata": {}, "cell_type": "code", "source": "# Import required libraries\nimport itertools\nimport os\nimport requests\nimport tarfile\nfrom collections import defaultdict\n\nimport string\nimport numpy as np\nimport pandas as pd\n\nimport nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.cluster import homogeneity_score, completeness_score, v_measure_score\n\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "[nltk_data] Downloading package punkt to /home/wsuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "# 1. Sentiment Analysis <a class=\"anchor\" id=\"sentiment\"></a>\nFor this section, we use [IBM's Debater - Sentiment Composition Lexicons](https://developer.ibm.com/exchanges/data/all/sentiment-composition-lexicons/) dataset. Our goal here is to take text and capture sentiment on the entire text as well as for each sentence in the text. We base our model on [this paper](https://www.aclweb.org/anthology/C18-1189.pdf) (by creators of this dataset). Our final model is section 1.5, which combines 1.2 to 1.4. So in the following subsections, we:\n\n* 1.1 Get Data Files Paths - We will extract paths of the cleaned files 'unigrams.csv' and 'bigrams.csv' from Part 1 as well as the rule files 'ADJECTIVES.xlsx' and 'SEMANTIC_CLASSES.xlsx' from the original dataset.\n* 1.2 Create method that uses 'unigrams.csv' to match unigrams to sentiment score\n* 1.3 Create method that uses 'bigrams.csv' to match bigrams to sentiment score\n* 1.4 Create method that uses the rules from Table 1 of the paper to produce sentiment scores. This method essentially matches bigrams to certain rules that produce a predicted polarity (positive or negative)\n* 1.5 Combines 1.2 to 1.4. \n    * We first get bigrams of the text. Then to determine the sentiment of each bigram we will:\n        1. Take the bigram score (1.3, calulate_bigram_sentiment()). If this does not exist then,\n        2. Take the score from matching component/adjective classes (1.4, calculate_compostion_or_adj_sentiment()). If this does not exist then,\n        3. Look at the unigrams (1.2, calulate_bigram_sentiment()) of the bigram. Both words need to be negative in order to be negative (similarly for positive). If one is positive and one negative then it is neutral\n        * Positive bigram score = +1, Negative bigram score = -1, Neutral bigram score = 0\n    * To determine final sentiment of a sentence, the sentiment score of each bigram is added together.\n    * To determine final sentiment of entire text, the sentiment score of each sentence is added together. \n* 1.6 Using 1.5, demonstrate examples of grouping comments by sentiment as well as hilighting sentiment on a sentence level."}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.1 Get Data Files Paths <a class=\"anchor\" id=\"1-1\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "We start by extracting the paths of the dataset that was stored to the project assets in the project notebook Part 1 - Data Exploration & Visualization as well as rule files 'ADJECTIVES.xlsx' and 'SEMANTIC_CLASSES.xlsx' from the original dataset.\n\nNote: if you haven't yet run this notebook, run it first; otherwise the cells below will not work."}, {"metadata": {}, "cell_type": "code", "source": "def get_file_handle(fname):\n    # Project data path for the raw data file\n    data_path = project.get_file(fname)\n    data_path.seek(0)\n    return data_path\n\n# Get paths of the files and use them in appropriate place\ndata_paths = [get_file_handle(file) for file in ['unigrams.csv', 'bigrams.csv', 'ADJECTIVES.xlsx', 'SEMANTIC_CLASSES.xlsx']]", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Comments for testing\ncomments_5 = [\n    'Customer service was polite.',\n    'The socks are a pretty color but expensive.',\n    'The shirt I bought was green and service was great.',\n    'I think the sweater and socks were perfect.',\n    'I do not like the shoes, so ugly and expensive.',\n]", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.2 Unigrams <a class=\"anchor\" id=\"1-2\"></a>\n\nIn this first implementation (calculate_unigram_sentiment()), we use the unigrams dataset, which contains the sentiment of various unigrams. Given an input sentence, we tokenize the sentence (break it up as a list of words) then match each word in the sentence to the words in the unigram dataset and find its sentiment value. If the word is not found in the dataset, it is skipped."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Read unigram data\nunigram_df = pd.read_csv(data_paths[0])\nprint(unigram_df.head())\n\n# Create dict with unigram and sentiment\nunigram_sentiment_dict = pd.Series(unigram_df.SENTIMENT_SCORE.values,\n                                   index=unigram_df.UNIGRAM.values).to_dict()", "execution_count": 5, "outputs": [{"output_type": "stream", "text": "   Unnamed: 0 UNIGRAM  SENTIMENT_SCORE  sentiment  uni_len first_letter\n0           0      aa         0.019674          1        2            a\n1           1     aaa         0.032775          1        3            a\n2           2    aaas         0.074593          1        4            a\n3           3  aachen         0.011926          1        6            a\n4           4     aah         0.118070          1        3            a\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "# Simple implementation: if finds token in unigram_sentiment_dict, then adds sentiment\n# The total sentiment for sentence is averaged\ndef calculate_unigram_sentiment(tokenized_sentence, sentiment_map=unigram_sentiment_dict):\n    sentiment_score = 0\n    for token in tokenized_sentence:\n        token_sentiment = sentiment_map.get(token)\n        if token_sentiment is None:\n            continue\n        else:\n            sentiment_score += token_sentiment\n\n    return sentiment_score", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Below is an example using this simple implementation."}, {"metadata": {}, "cell_type": "code", "source": "# Print out example sentences and their sentiment.\n# The more negative the score, the more negative the sentiment.\n# The more positive the score, the more positive the sentiment.\nfor sentence in comments_5:\n    score = calculate_unigram_sentiment(word_tokenize(sentence.lower()))\n    print('Score: {}, Sentence: {}'.format(score, sentence))", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "Score: 0.7981163, Sentence: Customer service was polite.\nScore: -0.26573172, Sentence: The socks are a pretty color but expensive.\nScore: 0.18411419999999995, Sentence: The shirt I bought was green and service was great.\nScore: 0.16159839999999998, Sentence: I think the sweater and socks were perfect.\nScore: -1.1203735, Sentence: I do not like the shoes, so ugly and expensive.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.3 Bigrams <a class=\"anchor\" id=\"1-3\"></a>\n\nThis second implementation (calculate_bigram_sentiment()) is similar to the first unigrams implementation however it uses bigrams instead. Given an input sentence, we tokenize the sentence into bigrams (e.g. \"the good dog\" becomes something like [(\"the\", \"good\"), (\"good\", \"dog\")]) then match each bigram in the sentence to the bigrams in the dataset and find its sentiment value. If a bigram is not found in the dataset, it is skipped."}, {"metadata": {}, "cell_type": "code", "source": "# Read bigram data\nbigrams_df = pd.read_csv(data_paths[1])\nbigrams_df.head()", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "   Unnamed: 0               BIGRAM POS_TAGS  SENTIMENT_SCORE  sentiment  \\\n0           0       abalone-divers   NN-NNS        -0.090230          0   \n1           1    abandoned-animals  VBN-NNS        -0.089895          0   \n2           2  abandoned-apartment   VBN-NN        -0.126907          0   \n3           3   abandoned-attempts  VBN-NNS        -0.053709          0   \n4           4     abandoned-babies  VBN-NNS        -0.074742          0   \n\n  first_letter  \n0            a  \n1            a  \n2            a  \n3            a  \n4            a  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>BIGRAM</th>\n      <th>POS_TAGS</th>\n      <th>SENTIMENT_SCORE</th>\n      <th>sentiment</th>\n      <th>first_letter</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>abalone-divers</td>\n      <td>NN-NNS</td>\n      <td>-0.090230</td>\n      <td>0</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>abandoned-animals</td>\n      <td>VBN-NNS</td>\n      <td>-0.089895</td>\n      <td>0</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>abandoned-apartment</td>\n      <td>VBN-NN</td>\n      <td>-0.126907</td>\n      <td>0</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>abandoned-attempts</td>\n      <td>VBN-NNS</td>\n      <td>-0.053709</td>\n      <td>0</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>abandoned-babies</td>\n      <td>VBN-NNS</td>\n      <td>-0.074742</td>\n      <td>0</td>\n      <td>a</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "bigram_sentiment_dict = pd.Series(bigrams_df.SENTIMENT_SCORE.values,\n                                   index=bigrams_df.BIGRAM.str.split('-').apply(lambda l: tuple(l))).to_dict()", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def calculate_bigram_sentiment_sentence(sentence_bigrams):\n    bigrams = []\n    bigram_sentiment_score = 0\n    for bigram in sentence_bigrams:\n        bigram_sentiment = calculate_bigram_sentiment(bigram)\n        if bigram_sentiment is not None:\n            bigram_sentiment_score += bigram_sentiment\n            bigrams.append(bigram)\n    \n    return bigram_sentiment_score, bigrams\n\n\ndef calculate_bigram_sentiment(bigram):\n    bigram_sentiment = bigram_sentiment_dict.get(bigram)\n    if bigram_sentiment:\n        # -0.02 and 0.02 allows for margin of error for neutrals\n        if bigram_sentiment < -0.02:\n            return -1\n        elif bigram_sentiment > 0.02:\n            return 1\n        else:\n            return 0\n    return None", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Below is an example using the bigram implementation."}, {"metadata": {}, "cell_type": "code", "source": "sentence = 'The high prices are ridiculous'\nsentence2 = 'They do not accept payments from my credit card'\ncalculate_bigram_sentiment_sentence(list(nltk.bigrams(word_tokenize(sentence.lower()))))\ncalculate_bigram_sentiment_sentence(list(nltk.bigrams(word_tokenize(sentence2.lower()))))", "execution_count": 11, "outputs": [{"output_type": "execute_result", "execution_count": 11, "data": {"text/plain": "(1, [('accept', 'payments')])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.4 Using Composition and Adjective Classes <a class=\"anchor\" id=\"1-4\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "### Composition and Adjective Class\n\nHere, we will uses the rules from Table 1 of [this paper](https://www.aclweb.org/anthology/C18-1189.pdf) (by creators of this dataset), to create our sentiment analysis model to produce sentiment scores. Additionally in the second notebook, we will essentially match bigrams to certain rules that produce a predicted polarity (positive or negative). There are two groups of rules: composition classes and adjective classes. Adjective classes focus on the adjective pairs (high, low) and (fast, slow).\n\nIn order to do this, there are two files: 1) ADJECTIVES.xlsx and 2) SEMANTIC_CLASSES.xlsx. The adjectives files contains 5 sheets, the first sheet gives a list of words similar to each of high, low, fast, slow. The next four sheets are words that are associated with that specific case.\n\nThe semantic classes file has 6 sheets, one for each of the composition classes defined in the paper. In each sheet, there is a list of words that corresponds to that composition class.\n\n#### SEMANTIC_CLASSES.xlsx: \nThis file contains the lists of the semantic classes words for each type. For each semantic class (reversers, propagators, and dominators) there are two tabs in the Excel file, one for a positive composition (POS) and one for negative composition (NEG). Overall there are 6 tabs: `DOMINATOR_NEG`, `DOMINATOR_POS`, `PROPAGETOR_POS`, `PROPAGETOR_NEG`, `REVERSER_POS`, `REVERSER_NEG`.\n\n#### ADJECTIVES.xlsx\nThis file contains the lists of the semantic classes words for the gradable adjective pairs.\n\n- `(HIGH,LOW)_POS_NEG`, `(HIGH,LOW)_NEG_POS`: the lists of words for ADJ high/low.\n- `(FAST,SLOW)_POS_NEG`, `(FAST,SLOW)_NEG_POS`: the lists of words for ADJ fast/slow.\n- `ADJECTIVE_EXPANSION`: the list of adjective expansions for high, low, fast, slow."}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4.1 Reading Adjective Classes Data Files\n\nFirst, we will read in ADJECTIVE_EXPANSION.xlsx and clean it up."}, {"metadata": {}, "cell_type": "code", "source": "xls_file = pd.ExcelFile(data_paths[2])\nadjective_expansion = pd.read_excel(xls_file, 'ADJECTIVE_EXPANSION').dropna(how='all').reset_index(drop=True)\nhigh_low_PN = pd.read_excel(xls_file, '(HIGH,LOW)_POS_NEG', header=None)[0].values.tolist()\nhigh_low_NP = pd.read_excel(xls_file, '(HIGH,LOW)_NEG_POS', header=None)[0].values.tolist()\nfast_slow_PN = pd.read_excel(xls_file, '(FAST,SLOW)_POS_NEG', header=None)[0].values.tolist()\nfast_slow_NP = pd.read_excel(xls_file, '(FAST,SLOW)_NEG_POS', header=None)[0].values.tolist()\n\n\ndef clean_adjective_df(df):\n    adjectives = []\n    for i in range(4):\n        adjectives.extend(df.iloc[:, i+1].dropna().tolist())\n    \n    adj_category = df.iloc[0,0]\n    \n    return [(adj, adj_category) for adj in adjectives]\n\n\ntokens = []\ntoken_rows = 5\nfor i in range(0, len(adjective_expansion), token_rows+1):\n    tokens.append(clean_adjective_df(adjective_expansion.loc[i:i+token_rows]))\nhigh_tokens, low_tokens, fast_tokens, slow_tokens = tokens\n\nadjective_class_map = dict(high_tokens + low_tokens + fast_tokens + slow_tokens)", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "set(adjective_class_map.values())", "execution_count": 13, "outputs": [{"output_type": "execute_result", "execution_count": 13, "data": {"text/plain": "{'FAST_TOKENS', 'HIGH_TOKENS', 'LOW_TOKENS', 'SLOW_TOKENS'}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we have a dictionary that matches words to the adjective class (fast, high, low, slow). The dictionary looks like:\n{word: adjective_class}"}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4.2 Reading Compostion Classes Data Files\n\nNext, we read the SEMANTIC_CLASSES.xlsx file"}, {"metadata": {}, "cell_type": "code", "source": "semantic_classes_file = pd.ExcelFile(data_paths[3])", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dominator_neg = pd.read_excel(semantic_classes_file, 'DOMINATOR_NEG', header=None)[0].values.tolist()\ndominator_pos = pd.read_excel(semantic_classes_file, 'DOMINATOR_POS', header=None)[0].values.tolist()\npropagator_pos = pd.read_excel(semantic_classes_file, 'PROPAGATOR_POS', header=None)[0].values.tolist()\npropagator_neg = pd.read_excel(semantic_classes_file, 'PROPAGATOR_NEG', header=None)[0].values.tolist()\nreverser_pos = pd.read_excel(semantic_classes_file, 'REVERSER_POS', header=None)[0].values.tolist()\nreverser_neg = pd.read_excel(semantic_classes_file, 'REVERSER_NEG', header=None)[0].values.tolist()", "execution_count": 15, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 1.4.3 Matching Adjective/Composition Classes\nNow we write a method to match bigrams to an adjective or composition class. The bigram matching order will be, as stated in the paper: ADJ (adjective), REV (reverse), PROP (propagator), DOM (dominator)"}, {"metadata": {}, "cell_type": "code", "source": "class Adjective:\n    FAST = 'FAST_TOKENS'\n    SLOW = 'SLOW_TOKENS'\n    HIGH = 'HIGH_TOKENS'\n    LOW = 'LOW_TOKENS'\n    \nclass Sign:\n    POSITIVE = 'Positive'\n    NEGATIVE = 'Negative'\n    NEUTRAL = 'Neutral'\n\nadjective_conditions = {\n    Adjective.FAST: [(fast_slow_PN, Sign.POSITIVE), (fast_slow_NP, Sign.NEGATIVE)],\n    Adjective.SLOW: [(fast_slow_PN, Sign.NEGATIVE), (fast_slow_NP, Sign.POSITIVE)],\n    Adjective.HIGH: [(high_low_PN, Sign.POSITIVE), (high_low_NP, Sign.NEGATIVE)],\n    Adjective.LOW: [(high_low_PN, Sign.NEGATIVE), (high_low_NP, Sign.POSITIVE)],\n}\n\nsentiment_to_score = {\n    Sign.POSITIVE: +1,\n    Sign.NEGATIVE: -1,\n}\n\ndef is_given_sentiment(sentiment, word, sentiment_map):\n    if word in sentiment_map:\n        if sentiment==Sign.NEGATIVE and sentiment_map[word] < 0:\n            return True\n        elif sentiment==Sign.POSITIVE and sentiment_map[word] > 0:\n            return True\n\ndef calculate_composition_or_adj_sentiment(bigram):\n    # Adjective\n    adjective_token = adjective_class_map.get(bigram[0])\n    if adjective_token is not None:\n        for expansions_list, sentiment_sign in adjective_conditions[adjective_token]:\n            if bigram[1] in expansions_list:\n                return sentiment_to_score[sentiment_sign]\n\n    # Composition: Reverser\n    elif bigram[0] in reverser_pos and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.POSITIVE]\n    elif bigram[0] in reverser_neg and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.NEGATIVE]\n\n    # Composition: Propagator\n    elif bigram[0] in propagator_pos and is_given_sentiment(Sign.NEGATIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.POSITIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.POSITIVE]\n    elif bigram[0] in propagator_neg and is_given_sentiment(Sign.POSITIVE, bigram[0], unigram_sentiment_dict) and is_given_sentiment(Sign.NEGATIVE, bigram[1], unigram_sentiment_dict):\n        return sentiment_to_score[Sign.NEGATIVE]\n\n    # Composition: Dominator\n    elif bigram[0] in dominator_neg:\n        return sentiment_to_score[Sign.NEGATIVE]\n    elif bigram[0] in dominator_pos:\n        return sentiment_to_score[Sign.POSITIVE]\n    \n    return None\n    \n        \ndef calculate_composition_or_adj_sentiment_sentence(sentence):\n    sentiment_count = 0\n    bigrams = []\n    sentence_bigrams = list(nltk.bigrams(word_tokenize(sentence.lower())))\n    for bigram in sentence_bigrams:\n        sentiment = calculate_composition_or_adj_sentiment(bigram)\n        if sentiment is not None:\n            sentiment_count += sentiment\n            bigrams.append(bigram)\n        else:\n            continue \n            \n    # if sentiment_count > 0 then positive\n    return sentiment_count, bigrams", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can now calculate the sentiment of sentences. For example:"}, {"metadata": {}, "cell_type": "code", "source": "calculate_composition_or_adj_sentiment_sentence(\"The high prices are ridiculous\")", "execution_count": 17, "outputs": [{"output_type": "execute_result", "execution_count": 17, "data": {"text/plain": "(-1, [('high', 'prices')])"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "A value < 0 means a negative sentiment and a value > 0 means positive. Thus, in the example above, that sentence has a negative sentiment. The tuple printed out is the bigram that was matched to determine the sentiment."}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.5 Combining Unigram, Bigram, Component/Adj Classes <a class=\"anchor\" id=\"1-5\"></a>\n\nIn this section, we combine the techniques from 1.1 to 1.3 to calculate sentiment. We will first get bigrams of the text. Then to determine the final sentiment of each bigram we will:\n1. Take the bigram score (1.3, calulate_bigram_sentiment()). If this does not exist then,\n2. Take the score from matching component/adj (1.4, calculate_compostion_or_adj_sentiment()). If this does not exist then,\n3. Look at the unigrams (1.2, calulate_bigram_sentiment()) of the bigram. Both words need to be negative in order to negative (similar for positive). If one is positive and one negative then it is neutral"}, {"metadata": {}, "cell_type": "code", "source": "def calculate_sentiment_combined(sentence):\n    sentiment_score = 0\n    table = str.maketrans(dict.fromkeys(string.punctuation))\n    cleaned_sentence = sentence.translate(table)  # remove punctuation\n    sentence_bigrams = list(nltk.bigrams(word_tokenize(cleaned_sentence.lower())))\n    \n    for bigram in sentence_bigrams:\n        current_sentiment = calculate_bigram_sentiment(bigram)\n        if current_sentiment is None:\n            current_sentiment = calculate_composition_or_adj_sentiment(bigram)\n            if current_sentiment is None:\n                unigram_sentiment = calculate_unigram_sentiment(bigram)\n                if unigram_sentiment < - 0.1:\n                    current_sentiment = -1\n                elif unigram_sentiment > 0.1:\n                    current_sentiment = 1\n                else:\n                    current_sentiment = 0\n        \n        sentiment_score += current_sentiment\n    \n    return sentiment_score\n    ", "execution_count": 18, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Example using the combined model. As stated previously, a score < 0 is negative sentiment, score > 0 is positive, and score = 0 is neutral."}, {"metadata": {}, "cell_type": "code", "source": "extra_sentences = ['The high prices are ridiculous', \n                   'They do not accept payments from my credit card',\n                   'I absolutely love how nice they are, I would definitely buy again from here.',\n                   'I hate their products, so horrible, I cannot believe I spent so much money on shoes.',\n                   'However, their webiste is beautiful and modern.',\n                  ]\nsentiment_sentences = comments_5 + extra_sentences\n\nfor sentence in sentiment_sentences:\n    score = calculate_sentiment_combined(sentence)\n    print('Score: {}, Sentence: {}'.format(score, sentence))", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Score: 2, Sentence: Customer service was polite.\nScore: -1, Sentence: The socks are a pretty color but expensive.\nScore: -1, Sentence: The shirt I bought was green and service was great.\nScore: -1, Sentence: I think the sweater and socks were perfect.\nScore: -3, Sentence: I do not like the shoes, so ugly and expensive.\nScore: -3, Sentence: The high prices are ridiculous\nScore: 0, Sentence: They do not accept payments from my credit card\nScore: 7, Sentence: I absolutely love how nice they are, I would definitely buy again from here.\nScore: -2, Sentence: I hate their products, so horrible, I cannot believe I spent so much money on shoes.\nScore: 3, Sentence: However, their webiste is beautiful and modern.\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 1.6 Group by Overall Sentiment <a class=\"anchor\" id=\"1-6\"></a>\n\nNow we add a method to group the entire comment's overall sentiment rather than just its individual sentences. Within each comment, we also label the positive/negative sentences."}, {"metadata": {}, "cell_type": "code", "source": "def convert_score_to_sentiment(score):\n    if score < 0:\n        return Sign.NEGATIVE\n    elif score > 0:\n        return Sign.POSITIVE\n    else:\n        return Sign.NEUTRAL\n    \n\ndef calculate_sentence_level_sentiment(comment_by_sentence):\n    \"\"\"\n    :param comment_by_sentence: comment broken down by sentence [sentence1, sentence2, ...]\n                                each sentence is string.\n    :return: (overall_score, [(sentiment, sentence), ...])\n    \"\"\"\n    sentence_level_sentiment = []\n    overall_score = 0\n    for sentence in comment_by_sentence:\n        score = calculate_sentiment_combined(sentence)\n        overall_score += score\n        sentiment_sentence_pair = (convert_score_to_sentiment(score), sentence)\n        sentence_level_sentiment.append(sentiment_sentence_pair)\n    return overall_score, sentence_level_sentiment\n    \n\ndef group_comments_by_sentiment(comments):\n    \"\"\"\n    :param comments: [comment, comment, ...]\n    :return: {Pos: [[(+/-, sentence1), (+/-, sentence2), ...], [...], [...], ...]\n              Neg: [], Neutral: [], }\n    \"\"\"\n    overall_sentiment = defaultdict(list)\n    for comment in comments:\n        comment_by_sentence = nltk.tokenize.sent_tokenize(comment)\n        overall_score, sentence_level_sentiment = calculate_sentence_level_sentiment(comment_by_sentence)\n        overall_sentiment[convert_score_to_sentiment(overall_score)].append(sentence_level_sentiment)\n\n    return overall_sentiment", "execution_count": 20, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Example\n\nIn the below example we visualize in a more human readable way the sentiment of each comment overall as well as each sentence in each comment.\n\nIn this example, two comments are overall negative, two comments overall are positive, and one is neutral."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "comments = [\n    'I bought several items: socks, shirt, sweater. By far my most favorite was the shirt because it is so soft. However, the sweater and socks missed the mark.',\n    'My order arrived several days late. But when I contacted customer serivce they were very helpful and refunded me.',\n    'Horrible, horrible customer service, I have never met such rude people. Why is it so bad? Would not recommend at all.',\n    'Everything I ordered arrived on time and looked exactly like in the pictures! This company has high quality products.',\n    'I bought somethings on sale, they were a great deal. Will be buying more next time.'\n]\n\noverall_sentiment = group_comments_by_sentiment(comments)\nsign_word_to_symbol = {Sign.NEGATIVE: '-', Sign.POSITIVE: '+', Sign.NEUTRAL: '='}\n\n# print out results in readible way\nfor sign in overall_sentiment:\n    print('{}:'.format(sign))\n    for comment in overall_sentiment[sign]:\n        sentence_printout_text = ''\n        for sentence_sign, sentence in comment:\n            sentence_printout_text += '({}) {}\\n'.format(sign_word_to_symbol[sentence_sign], sentence)\n        print(sentence_printout_text)", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "Negative:\n(-) I bought several items: socks, shirt, sweater.\n(-) By far my most favorite was the shirt because it is so soft.\n(-) However, the sweater and socks missed the mark.\n\n(-) Horrible, horrible customer service, I have never met such rude people.\n(-) Why is it so bad?\n(-) Would not recommend at all.\n\nPositive:\n(-) My order arrived several days late.\n(+) But when I contacted customer serivce they were very helpful and refunded me.\n\n(+) I bought somethings on sale, they were a great deal.\n(=) Will be buying more next time.\n\nNeutral:\n(-) Everything I ordered arrived on time and looked exactly like in the pictures!\n(+) This company has high quality products.\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"authors\"></a> \n### Authors\nThis notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n\nCopyright \u00a9 2020 IBM. This notebook and its source code are released under the terms of the MIT License."}, {"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}