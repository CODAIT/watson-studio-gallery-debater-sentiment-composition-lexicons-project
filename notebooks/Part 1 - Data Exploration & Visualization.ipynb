{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='f50cd9d7-15ac-4ff4-8ca0-66ac443fd868', project_access_token='p-9fb965e91b44d582783cfe3da9a3cc430495311f')\n",
    "pc = project.project_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Visualize IBM Debater® Sentiment Composition Lexicons\n",
    "This notebook relates to the IBM Debater® Sentiment Composition Lexicons dataset. The dataset includes sentiment composition lexicons and sentiment lexicons:\n",
    "1. Sentiment composition lexicons containing 2,783 words.\n",
    "2. Sentiment lexicons containing 66,058 unigrams and 262,555 bigrams.\n",
    "\n",
    "This dataset can be obtained for free from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/sentiment-composition-lexicons/).\n",
    "\n",
    "In this notebook, we load, explore, clean and visualize the dataset.\n",
    "\n",
    "This dataset addresses sentiment composition – predicting the sentiment of a phrase from the interaction between its constituents. For example, in the phrases “reduced bureaucracy” and “fresh injury”, both “reduced” and “fresh” are followed by a negative word. However, “reduced” flips the negative polarity, resulting in a positive phrase, while “fresh” propagates the negative polarity to the phrase level, resulting in a negative phrase. Accordingly, “reduced” is part of our “reversers” lexicon, and “fresh” is part of the “propagators” lexicon.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [0. Prerequisite](#prerequisite)\n",
    "* [1. Load Data](#1)   \n",
    "    * [1.1 About](#abstract)\n",
    "    * [1.2 Download and Extract](#download)\n",
    "* [2. Data Visualization](#2)\n",
    "* [3. Save the Cleaned Data](#3)\n",
    "* [Authors](#authors)\n",
    "\n",
    "\n",
    "<a class=\"anchor\" id=\"prerequisite\"></a>\n",
    "### 0. Prerequisites\n",
    "\n",
    "Before you run this notebook complete the following steps:\n",
    "- Insert a project token\n",
    "- Import required modules\n",
    "\n",
    "#### Insert a project token\n",
    "\n",
    "When you import this project from the Watson Studio Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n",
    "\n",
    "```python\n",
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "from project_lib import Project\n",
    "project = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\n",
    "pc = project.project_context\n",
    "```\n",
    "\n",
    "If you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n",
    "\n",
    "* Click on `More -> Insert project token` in the top-right menu section\n",
    "\n",
    "![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n",
    "\n",
    "* This should insert a cell at the top of this notebook similar to the example given above.\n",
    "\n",
    "  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n",
    "\n",
    "* Run the newly inserted cell before proceeding with the notebook execution below\n",
    "\n",
    "#### Import required modules\n",
    "\n",
    "Import and configure the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define required imports\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import seaborn as sns\n",
    "!pip install cufflinks\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data <a class=\"anchor\" id=\"#1\"></a>\n",
    "### 1.1 About <a class=\"anchor\" id=\"abstract\"></a>\n",
    "\n",
    "The goal of these notebooks is to use the [IBM's Debater - Sentiment Composition Lexicons](https://developer.ibm.com/exchanges/data/all/sentiment-composition-lexicons/) dataset to categorize text on a sentence level, or as a whole, with a range of sentiments. This could be used in for example an application that collects comments and feedback from customers of a company to determine which customers are more satisfied with the company.\n",
    "\n",
    "Let's first explain a few definition\n",
    "Sentiment Analysis: Using natural language processing to determine the sentiment of a piece of text, e.g. determine if the text has a positive, negative, or neutral connotation. \n",
    "\n",
    "N-gram: a sequence of N terms\n",
    "\n",
    "Unigram: an N-gram with one term, e.g. “hello”\n",
    "\n",
    "Bi-gram: an N-gram with two terms, e.g. “hello world”\n",
    "\n",
    "POS (part of speech) tag: a word's part of speech, e.g. the POS tag for \"dog\" would be noun (or NN in the NLTK Python library).\n",
    "\n",
    "### 1.2 Download and Extract <a class=\"anchor\" id=\"download\"></a>\n",
    "\n",
    "First, we must load then modify the LEXICON_UG.txt and LEXICON_BG.txt datasets to include a sentiment column that is based on the SENTIMENT_SCORE column but instead uses 1 or 0 where 1 is positive sentiment and 0 is negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get data file function\n",
    "def get_file_handle(fname):\n",
    "    # Project data path for the raw data file\n",
    "    data_path = project.get_file(fname)\n",
    "    data_path.seek(0)\n",
    "    return data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEXICON_UG.txt: \n",
    "A list of 66,058 unigrams and their predicted sentiment score. Note that in the paper, for unigrams that have sentiment in the HL lexicon (the publicly-available sentiment lexicon of Hu and Liu (2004)), we used the original sentiment from the HL lexicon (+1 or -1) and not the predicted score. This step is not reflected in the released lexicon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filename\n",
    "DATA_PATH = 'LEXICON_UG.txt'\n",
    "\n",
    "# Using pandas to read the data \n",
    "# Since the `DATE` column consists date-time information, we use Pandas parse_dates keyword for easier data processing\n",
    "data_path = get_file_handle(DATA_PATH)\n",
    "unigrams = pd.read_csv(data_path, sep=\" \")\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LEXICON_BG.txt: \n",
    "A list of 262,555 selected bigrams in the following format:\n",
    "- Column 1: the bigram\n",
    "- Column 2: the OpenNLP POS tags of its unigrams\n",
    "- Column 3: the predicted sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define filename\n",
    "DATA_PATH = 'LEXICON_BG.txt'\n",
    "\n",
    "# Using pandas to read the data \n",
    "# Since the `DATE` column consists date-time information, we use Pandas parse_dates keyword for easier data processing\n",
    "data_path = get_file_handle(DATA_PATH)\n",
    "bigrams = pd.read_csv(data_path, sep=\" \")\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition and Adjective Class\n",
    "\n",
    "In Part 2, we follow the rules from Table 1 of [this paper](https://www.aclweb.org/anthology/C18-1189.pdf) (by creators of this dataset), to create our sentiment analysis model to produce sentiment scores. Additionally in the second notebook, we will essentially match bigrams to certain rules that produce a predicted polarity (positive or negative). There are two groups of rules: composition classes and adjective classes. Adjective classes focus on the adjective pairs (high, low) and (fast, slow).\n",
    "\n",
    "In order to do this, there are two files: 1) ADJECTIVES.xlsx and 2) SEMANTIC_CLASSES.xlsx. The adjectives files contains 5 sheets, the first sheet gives a list of words similar to each of high, low, fast, slow. The next four sheets are words that are associated with that specific case.\n",
    "\n",
    "The semantic classes file has 6 sheets, one for each of the composition classes defined in the paper. In each sheet, there is a list of words that corresponds to that composition class.\n",
    "\n",
    "#### SEMANTIC_CLASSES.xlsx: \n",
    "This file contains the lists of the semantic classes words for each type. For each semantic class (reversers, propagators, and dominators) there are two tabs in the Excel file, one for a positive composition (POS) and one for negative composition (NEG). Overall there are 6 tabs: `DOMINATOR_NEG`, `DOMINATOR_POS`, `PROPAGETOR_POS`, `PROPAGETOR_NEG`, `REVERSER_POS`, `REVERSER_NEG`.\n",
    "\n",
    "#### ADJECTIVES.xlsx\n",
    "This file contains the lists of the semantic classes words for the gradable adjective pairs.\n",
    "\n",
    "- `(HIGH,LOW)_POS_NEG`, `(HIGH,LOW)_NEG_POS`: the lists of words for ADJ high/low.\n",
    "- `(FAST,SLOW)_POS_NEG`, `(FAST,SLOW)_NEG_POS`: the lists of words for ADJ fast/slow.\n",
    "- `ADJECTIVE_EXPANSION`: the list of adjective expansions for high, low, fast, slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Visualization <a class=\"anchor\" id=\"2\"></a>\n",
    "#### 2.2 Unigrams <a class=\"anchor\" id=\"2-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add sentiment column\n",
    "unigrams['sentiment'] = np.where(unigrams['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The distribution of review sentiment polarity score\n",
    "unigrams['SENTIMENT_SCORE'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='polarity',\n",
    "    linecolor='black',\n",
    "    yTitle='count',\n",
    "    title='Sentiment Polarity Distribution')\n",
    "# The sentiment polarity score is similar to a bell curve, center at 0, means half of them are positive, half are negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the string length of each unigram word\n",
    "unigrams['uni_len'] = [len(str(i)) for i in unigrams['UNIGRAM']]\n",
    "# Plot the unigram length distribution\n",
    "unigrams['uni_len'].iplot(\n",
    "    kind='hist',\n",
    "    xTitle='unigram length',\n",
    "    linecolor='black',\n",
    "    yTitle='count',\n",
    "    title='Unigram Text Length Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams['first_letter'] = unigrams.UNIGRAM.str[0]\n",
    "unigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of words under each alphabet\n",
    "group_data = unigrams.groupby(['first_letter', 'sentiment'])\n",
    "group_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"first_letter\", data=unigrams)\n",
    "\n",
    "plt.title('Data Distribution')\n",
    "\n",
    "for p in ax.patches:\n",
    "        total_count = str(p.get_height())\n",
    "        x=p.get_x() + p.get_width() - 0.75\n",
    "        y=p.get_y() +p.get_height()\n",
    "        ax.annotate(total_count, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the unigrams dataset, the first letter normally starts with s or c while the least frequent letters are x, y, and z.\n",
    "\n",
    "#### 2.3 Bigrams <a class=\"anchor\" id=\"2-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment column\n",
    "bigrams['sentiment'] = np.where(bigrams['SENTIMENT_SCORE'] > 0, 1, 0)  # 1 is positive, 0 is negative\n",
    "bigrams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The distribution of review sentiment polarity score\n",
    "bigrams['SENTIMENT_SCORE'].iplot(\n",
    "    kind='hist',\n",
    "    bins=50,\n",
    "    xTitle='polarity',\n",
    "    linecolor='black',\n",
    "    yTitle='count',\n",
    "    title='Sentiment Polarity Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bigrams.groupby(['POS_TAGS', 'sentiment']).size().reset_index())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(df, x=\"POS_TAGS\", y=0, color=\"sentiment\", title=\"Long-Form Input\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.groupby('POS_TAGS').count()['SENTIMENT_SCORE'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', opacity=0.8,\n",
    "                                                           title='Pos Tags Count', xTitle='Pos Tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of words under each POS Tag\n",
    "group_data = bigrams.groupby(['POS_TAGS','sentiment'])\n",
    "group_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add first letter and sentiment columns\n",
    "bigrams['first_letter'] = bigrams.BIGRAM.str[0]\n",
    "# get number of words under each alphabet\n",
    "group_data = bigrams.groupby(['first_letter','sentiment'])\n",
    "group_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sns.set(style=\"darkgrid\")\n",
    "ax = sns.countplot(x=\"first_letter\", data=bigrams)\n",
    "\n",
    "plt.title('Data Distribution')\n",
    "\n",
    "for p in ax.patches:\n",
    "        #total_count = '{}'.format(p.get_height())\n",
    "        total_count = str(p.get_height())\n",
    "        x=p.get_x() + p.get_width() - 0.75\n",
    "        y=p.get_y() +p.get_height()\n",
    "        ax.annotate(total_count, (x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Save the Cleaned Data\n",
    "\n",
    "Finally, we save the cleaned dataset as a Project asset for later re-use. You should see an output like the one below if successful:\n",
    "\n",
    "```\n",
    "{'file_name': 'bigrams.csv',\n",
    " 'message': 'File saved to project storage.',\n",
    " 'bucket_name': 'ibmdebatersentimentcompositionlex-donotdelete-pr-jhjwrb2ah5iwb0',\n",
    " 'asset_id': '644d1e6c-757e-401c-9ff8-f6090e5ac998'}\n",
    "```\n",
    "\n",
    "**Note**: In order for this step to work, your project token (see the first cell of this notebook) must have `Editor` role. By default this will overwrite any existing file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_data(\"unigrams.csv\", unigrams.to_csv(float_format='%g'), overwrite=True)\n",
    "project.save_data(\"bigrams.csv\", bigrams.to_csv(float_format='%g'), overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Next steps\n",
    "\n",
    "- Close this notebook.\n",
    "- Open the `Part 2 - Model Development` notebook to explore the cleaned dataset.\n",
    "\n",
    "<a id=\"authors\"></a> \n",
    "### Authors\n",
    "This notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n",
    "\n",
    "Copyright © 2020 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
